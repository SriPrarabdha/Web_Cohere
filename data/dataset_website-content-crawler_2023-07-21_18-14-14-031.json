[{
  "url": "https://python.langchain.com/en/latest/",
  "crawl": {
    "loadedUrl": "https://python.langchain.com/docs/get_started/introduction.html",
    "loadedTime": "2023-07-21T18:13:32.895Z",
    "referrerUrl": "https://python.langchain.com/en/latest/",
    "depth": 0,
    "httpStatusCode": 200
  },
  "metadata": {
    "canonicalUrl": "https://python.langchain.com/docs/get_started/introduction.html",
    "title": "Introduction | ðŸ¦œï¸ðŸ”— Langchain",
    "description": "LangChain is a framework for developing applications powered by language models. It enables applications that are:",
    "author": null,
    "keywords": null,
    "languageCode": "en"
  },
  "screenshotUrl": null,
  "text": "Introduction\nLangChain is a framework for developing applications powered by language models. It enables applications that are:\nData-aware: connect a language model to other sources of data\nAgentic: allow a language model to interact with its environment\nThe main value props of LangChain are:\nComponents: abstractions for working with language models, along with a collection of implementations for each abstraction. Components are modular and easy-to-use, whether you are using the rest of the LangChain framework or not\nOff-the-shelf chains: a structured assembly of components for accomplishing specific higher-level tasks\nOff-the-shelf chains make it easy to get started. For more complex applications and nuanced use-cases, components make it easy to customize existing chains or build new ones.\nGet startedâ€‹\nHereâ€™s how to install LangChain, set up your environment, and start building.\nWe recommend following our Quickstart guide to familiarize yourself with the framework by building your first LangChain application.\nNote: These docs are for the LangChain Python package. For documentation on LangChain.js, the JS/TS version, head here.\nModulesâ€‹\nLangChain provides standard, extendable interfaces and external integrations for the following modules, listed from least to most complex:\nModel I/Oâ€‹\nInterface with language models\nData connectionâ€‹\nInterface with application-specific data\nChainsâ€‹\nConstruct sequences of calls\nAgentsâ€‹\nLet chains choose which tools to use given high-level directives\nMemoryâ€‹\nPersist application state between runs of a chain\nCallbacksâ€‹\nLog and stream intermediate steps of any chain\nExamples, ecosystem, and resourcesâ€‹\nUse casesâ€‹\nWalkthroughs and best-practices for common end-to-end use cases, like:\nChatbots\nAnswering questions using sources\nAnalyzing structured data\nand much more...\nGuidesâ€‹\nLearn best practices for developing with LangChain.\nEcosystemâ€‹\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. Check out our growing list of integrations and dependent repos.\nAdditional resourcesâ€‹\nOur community is full of prolific developers, creative builders, and fantastic teachers. Check out YouTube tutorials for great tutorials from folks in the community, and Gallery for a list of awesome LangChain projects, compiled by the folks at KyroLabs.\nSupport \nJoin us on GitHub or Discord to ask questions, share feedback, meet other developers building with LangChain, and dream about the future of LLMâ€™s.\nAPI referenceâ€‹\nHead to the reference section for full documentation of all classes and methods in the LangChain Python package.",
  "markdown": "## Introduction\n\n**LangChain** is a framework for developing applications powered by language models. It enables applications that are:\n\n*   **Data-aware**: connect a language model to other sources of data\n*   **Agentic**: allow a language model to interact with its environment\n\nThe main value props of LangChain are:\n\n1.  **Components**: abstractions for working with language models, along with a collection of implementations for each abstraction. Components are modular and easy-to-use, whether you are using the rest of the LangChain framework or not\n2.  **Off-the-shelf chains**: a structured assembly of components for accomplishing specific higher-level tasks\n\nOff-the-shelf chains make it easy to get started. For more complex applications and nuanced use-cases, components make it easy to customize existing chains or build new ones.\n\n## Get started[â€‹](#get-started \"Direct link to Get started\")\n\n[Hereâ€™s](https://python.langchain.com/docs/get_started/installation.html) how to install LangChain, set up your environment, and start building.\n\nWe recommend following our [Quickstart](https://python.langchain.com/docs/get_started/quickstart.html) guide to familiarize yourself with the framework by building your first LangChain application.\n\n_**Note**: These docs are for the LangChain [Python package](https://github.com/hwchase17/langchain). For documentation on [LangChain.js](https://github.com/hwchase17/langchainjs), the JS/TS version, [head here](https://js.langchain.com/docs)._\n\n## Modules[â€‹](#modules \"Direct link to Modules\")\n\nLangChain provides standard, extendable interfaces and external integrations for the following modules, listed from least to most complex:\n\n#### [Model I/O](https://python.langchain.com/docs/modules/model_io/)[â€‹](#model-io \"Direct link to model-io\")\n\nInterface with language models\n\n#### [Data connection](https://python.langchain.com/docs/modules/data_connection/)[â€‹](#data-connection \"Direct link to data-connection\")\n\nInterface with application-specific data\n\n#### [Chains](https://python.langchain.com/docs/modules/chains/)[â€‹](#chains \"Direct link to chains\")\n\nConstruct sequences of calls\n\n#### [Agents](https://python.langchain.com/docs/modules/agents/)[â€‹](#agents \"Direct link to agents\")\n\nLet chains choose which tools to use given high-level directives\n\n#### [Memory](https://python.langchain.com/docs/modules/memory/)[â€‹](#memory \"Direct link to memory\")\n\nPersist application state between runs of a chain\n\n#### [Callbacks](https://python.langchain.com/docs/modules/callbacks/)[â€‹](#callbacks \"Direct link to callbacks\")\n\nLog and stream intermediate steps of any chain\n\n## Examples, ecosystem, and resources[â€‹](#examples-ecosystem-and-resources \"Direct link to Examples, ecosystem, and resources\")\n\n### [Use cases](https://python.langchain.com/docs/use_cases/)[â€‹](#use-cases \"Direct link to use-cases\")\n\nWalkthroughs and best-practices for common end-to-end use cases, like:\n\n*   [Chatbots](https://python.langchain.com/docs/use_cases/chatbots/)\n*   [Answering questions using sources](https://python.langchain.com/docs/use_cases/question_answering/)\n*   [Analyzing structured data](https://python.langchain.com/docs/use_cases/tabular.html)\n*   and much more...\n\n### [Guides](https://python.langchain.com/docs/guides/)[â€‹](#guides \"Direct link to guides\")\n\nLearn best practices for developing with LangChain.\n\n### [Ecosystem](https://python.langchain.com/docs/ecosystem/)[â€‹](#ecosystem \"Direct link to ecosystem\")\n\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. Check out our growing list of [integrations](https://python.langchain.com/docs/ecosystem/integrations/) and [dependent repos](https://python.langchain.com/docs/ecosystem/dependents.html).\n\n### [Additional resources](https://python.langchain.com/docs/additional_resources/)[â€‹](#additional-resources \"Direct link to additional-resources\")\n\nOur community is full of prolific developers, creative builders, and fantastic teachers. Check out [YouTube tutorials](https://python.langchain.com/docs/additional_resources/youtube.html) for great tutorials from folks in the community, and [Gallery](https://github.com/kyrolabs/awesome-langchain) for a list of awesome LangChain projects, compiled by the folks at [KyroLabs](https://kyrolabs.com/).\n\n### Support\n\nJoin us on [GitHub](https://github.com/hwchase17/langchain) or [Discord](https://discord.gg/6adMQxSpJS) to ask questions, share feedback, meet other developers building with LangChain, and dream about the future of LLMâ€™s.\n\n## API reference[â€‹](#api-reference \"Direct link to API reference\")\n\nHead to the [reference](https://api.python.langchain.com/) section for full documentation of all classes and methods in the LangChain Python package."
},
{
  "url": "https://python.langchain.com/docs/get_started/introduction",
  "crawl": {
    "loadedUrl": "https://python.langchain.com/docs/get_started/introduction",
    "loadedTime": "2023-07-21T18:13:43.381Z",
    "referrerUrl": "https://python.langchain.com/en/latest/",
    "depth": 1,
    "httpStatusCode": 200
  },
  "metadata": {
    "canonicalUrl": "https://python.langchain.com/docs/get_started/introduction",
    "title": "Introduction | ðŸ¦œï¸ðŸ”— Langchain",
    "description": "LangChain is a framework for developing applications powered by language models. It enables applications that are:",
    "author": null,
    "keywords": null,
    "languageCode": "en"
  },
  "screenshotUrl": null,
  "text": "Introduction\nLangChain is a framework for developing applications powered by language models. It enables applications that are:\nData-aware: connect a language model to other sources of data\nAgentic: allow a language model to interact with its environment\nThe main value props of LangChain are:\nComponents: abstractions for working with language models, along with a collection of implementations for each abstraction. Components are modular and easy-to-use, whether you are using the rest of the LangChain framework or not\nOff-the-shelf chains: a structured assembly of components for accomplishing specific higher-level tasks\nOff-the-shelf chains make it easy to get started. For more complex applications and nuanced use-cases, components make it easy to customize existing chains or build new ones.\nGet startedâ€‹\nHereâ€™s how to install LangChain, set up your environment, and start building.\nWe recommend following our Quickstart guide to familiarize yourself with the framework by building your first LangChain application.\nNote: These docs are for the LangChain Python package. For documentation on LangChain.js, the JS/TS version, head here.\nModulesâ€‹\nLangChain provides standard, extendable interfaces and external integrations for the following modules, listed from least to most complex:\nModel I/Oâ€‹\nInterface with language models\nData connectionâ€‹\nInterface with application-specific data\nChainsâ€‹\nConstruct sequences of calls\nAgentsâ€‹\nLet chains choose which tools to use given high-level directives\nMemoryâ€‹\nPersist application state between runs of a chain\nCallbacksâ€‹\nLog and stream intermediate steps of any chain\nExamples, ecosystem, and resourcesâ€‹\nUse casesâ€‹\nWalkthroughs and best-practices for common end-to-end use cases, like:\nChatbots\nAnswering questions using sources\nAnalyzing structured data\nand much more...\nGuidesâ€‹\nLearn best practices for developing with LangChain.\nEcosystemâ€‹\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. Check out our growing list of integrations and dependent repos.\nAdditional resourcesâ€‹\nOur community is full of prolific developers, creative builders, and fantastic teachers. Check out YouTube tutorials for great tutorials from folks in the community, and Gallery for a list of awesome LangChain projects, compiled by the folks at KyroLabs.\nSupport \nJoin us on GitHub or Discord to ask questions, share feedback, meet other developers building with LangChain, and dream about the future of LLMâ€™s.\nAPI referenceâ€‹\nHead to the reference section for full documentation of all classes and methods in the LangChain Python package.",
  "markdown": "## Introduction\n\n**LangChain** is a framework for developing applications powered by language models. It enables applications that are:\n\n*   **Data-aware**: connect a language model to other sources of data\n*   **Agentic**: allow a language model to interact with its environment\n\nThe main value props of LangChain are:\n\n1.  **Components**: abstractions for working with language models, along with a collection of implementations for each abstraction. Components are modular and easy-to-use, whether you are using the rest of the LangChain framework or not\n2.  **Off-the-shelf chains**: a structured assembly of components for accomplishing specific higher-level tasks\n\nOff-the-shelf chains make it easy to get started. For more complex applications and nuanced use-cases, components make it easy to customize existing chains or build new ones.\n\n## Get started[â€‹](#get-started \"Direct link to Get started\")\n\n[Hereâ€™s](https://python.langchain.com/docs/get_started/installation.html) how to install LangChain, set up your environment, and start building.\n\nWe recommend following our [Quickstart](https://python.langchain.com/docs/get_started/quickstart.html) guide to familiarize yourself with the framework by building your first LangChain application.\n\n_**Note**: These docs are for the LangChain [Python package](https://github.com/hwchase17/langchain). For documentation on [LangChain.js](https://github.com/hwchase17/langchainjs), the JS/TS version, [head here](https://js.langchain.com/docs)._\n\n## Modules[â€‹](#modules \"Direct link to Modules\")\n\nLangChain provides standard, extendable interfaces and external integrations for the following modules, listed from least to most complex:\n\n#### [Model I/O](https://python.langchain.com/docs/modules/model_io/)[â€‹](#model-io \"Direct link to model-io\")\n\nInterface with language models\n\n#### [Data connection](https://python.langchain.com/docs/modules/data_connection/)[â€‹](#data-connection \"Direct link to data-connection\")\n\nInterface with application-specific data\n\n#### [Chains](https://python.langchain.com/docs/modules/chains/)[â€‹](#chains \"Direct link to chains\")\n\nConstruct sequences of calls\n\n#### [Agents](https://python.langchain.com/docs/modules/agents/)[â€‹](#agents \"Direct link to agents\")\n\nLet chains choose which tools to use given high-level directives\n\n#### [Memory](https://python.langchain.com/docs/modules/memory/)[â€‹](#memory \"Direct link to memory\")\n\nPersist application state between runs of a chain\n\n#### [Callbacks](https://python.langchain.com/docs/modules/callbacks/)[â€‹](#callbacks \"Direct link to callbacks\")\n\nLog and stream intermediate steps of any chain\n\n## Examples, ecosystem, and resources[â€‹](#examples-ecosystem-and-resources \"Direct link to Examples, ecosystem, and resources\")\n\n### [Use cases](https://python.langchain.com/docs/use_cases/)[â€‹](#use-cases \"Direct link to use-cases\")\n\nWalkthroughs and best-practices for common end-to-end use cases, like:\n\n*   [Chatbots](https://python.langchain.com/docs/use_cases/chatbots/)\n*   [Answering questions using sources](https://python.langchain.com/docs/use_cases/question_answering/)\n*   [Analyzing structured data](https://python.langchain.com/docs/use_cases/tabular.html)\n*   and much more...\n\n### [Guides](https://python.langchain.com/docs/guides/)[â€‹](#guides \"Direct link to guides\")\n\nLearn best practices for developing with LangChain.\n\n### [Ecosystem](https://python.langchain.com/docs/ecosystem/)[â€‹](#ecosystem \"Direct link to ecosystem\")\n\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. Check out our growing list of [integrations](https://python.langchain.com/docs/ecosystem/integrations/) and [dependent repos](https://python.langchain.com/docs/ecosystem/dependents.html).\n\n### [Additional resources](https://python.langchain.com/docs/additional_resources/)[â€‹](#additional-resources \"Direct link to additional-resources\")\n\nOur community is full of prolific developers, creative builders, and fantastic teachers. Check out [YouTube tutorials](https://python.langchain.com/docs/additional_resources/youtube.html) for great tutorials from folks in the community, and [Gallery](https://github.com/kyrolabs/awesome-langchain) for a list of awesome LangChain projects, compiled by the folks at [KyroLabs](https://kyrolabs.com/).\n\n### Support\n\nJoin us on [GitHub](https://github.com/hwchase17/langchain) or [Discord](https://discord.gg/6adMQxSpJS) to ask questions, share feedback, meet other developers building with LangChain, and dream about the future of LLMâ€™s.\n\n## API reference[â€‹](#api-reference \"Direct link to API reference\")\n\nHead to the [reference](https://api.python.langchain.com/) section for full documentation of all classes and methods in the LangChain Python package."
},
{
  "url": "https://python.langchain.com/docs/get_started/installation",
  "crawl": {
    "loadedUrl": "https://python.langchain.com/docs/get_started/installation",
    "loadedTime": "2023-07-21T18:13:43.084Z",
    "referrerUrl": "https://python.langchain.com/en/latest/",
    "depth": 1,
    "httpStatusCode": 200
  },
  "metadata": {
    "canonicalUrl": "https://python.langchain.com/docs/get_started/installation",
    "title": "Installation | ðŸ¦œï¸ðŸ”— Langchain",
    "description": null,
    "author": null,
    "keywords": null,
    "languageCode": "en"
  },
  "screenshotUrl": null,
  "text": "Installation\nOfficial releaseâ€‹\nTo install LangChain run:\nPip\nConda\nThat will install the bare minimum requirements of LangChain. A lot of the value of LangChain comes when integrating it with various model providers, datastores, etc. By default, the dependencies needed to do that are NOT installed. However, there are two other ways to install LangChain that do bring in those dependencies.\nTo install modules needed for the common LLM providers, run:\npip install langchain[llms]\nTo install all modules needed for all integrations, run:\npip install langchain[all]\nNote that if you are using zsh, you'll need to quote square brackets when passing them as an argument to a command, for example:\npip install 'langchain[all]'\nFrom sourceâ€‹\nIf you want to install from source, you can do so by cloning the repo and running:",
  "markdown": "## Installation\n\n## Official release[â€‹](#official-release \"Direct link to Official release\")\n\nTo install LangChain run:\n\n*   Pip\n*   Conda\n\nThat will install the bare minimum requirements of LangChain. A lot of the value of LangChain comes when integrating it with various model providers, datastores, etc. By default, the dependencies needed to do that are NOT installed. However, there are two other ways to install LangChain that do bring in those dependencies.\n\nTo install modules needed for the common LLM providers, run:\n\n```\npip install langchain[llms]\n```\n\nTo install all modules needed for all integrations, run:\n\n```\npip install langchain[all]\n```\n\nNote that if you are using `zsh`, you'll need to quote square brackets when passing them as an argument to a command, for example:\n\n```\npip install 'langchain[all]'\n```\n\n## From source[â€‹](#from-source \"Direct link to From source\")\n\nIf you want to install from source, you can do so by cloning the repo and running:"
},
{
  "url": "https://python.langchain.com/docs/get_started/quickstart",
  "crawl": {
    "loadedUrl": "https://python.langchain.com/docs/get_started/quickstart",
    "loadedTime": "2023-07-21T18:13:51.381Z",
    "referrerUrl": "https://python.langchain.com/en/latest/",
    "depth": 1,
    "httpStatusCode": 200
  },
  "metadata": {
    "canonicalUrl": "https://python.langchain.com/docs/get_started/quickstart",
    "title": "Quickstart | ðŸ¦œï¸ðŸ”— Langchain",
    "description": "Installation",
    "author": null,
    "keywords": null,
    "languageCode": "en"
  },
  "screenshotUrl": null,
  "text": "Quickstart\nInstallationâ€‹\nTo install LangChain run:\nPip\nConda\nFor more details, see our Installation guide.\nEnvironment setupâ€‹\nUsing LangChain will usually require integrations with one or more model providers, data stores, APIs, etc. For this example, we'll use OpenAI's model APIs.\nFirst we'll need to install their Python package:\nAccessing the API requires an API key, which you can get by creating an account and heading here. Once we have a key we'll want to set it as an environment variable by running:\nexport OPENAI_API_KEY=\"...\"\nIf you'd prefer not to set an environment variable you can pass the key in directly via the openai_api_key named parameter when initiating the OpenAI LLM class:\nfrom langchain.llms import OpenAI\n\nllm = OpenAI(openai_api_key=\"...\")\nBuilding an applicationâ€‹\nNow we can start building our language model application. LangChain provides many modules that can be used to build language model applications. Modules can be used as stand-alones in simple applications and they can be combined for more complex use cases.\nLLMsâ€‹\nGet predictions from a language modelâ€‹\nThe basic building block of LangChain is the LLM, which takes in text and generates more text.\nAs an example, suppose we're building an application that generates a company name based on a company description. In order to do this, we need to initialize an OpenAI model wrapper. In this case, since we want the outputs to be MORE random, we'll initialize our model with a HIGH temperature.\nfrom langchain.llms import OpenAI\n\nllm = OpenAI(temperature=0.9)\nAnd now we can pass in text and get predictions!\nllm.predict(\"What would be a good company name for a company that makes colorful socks?\")\n# >> Feetful of Fun\nChat modelsâ€‹\nChat models are a variation on language models. While chat models use language models under the hood, the interface they expose is a bit different: rather than expose a \"text in, text out\" API, they expose an interface where \"chat messages\" are the inputs and outputs.\nYou can get chat completions by passing one or more messages to the chat model. The response will be a message. The types of messages currently supported in LangChain are AIMessage, HumanMessage, SystemMessage, and ChatMessage -- ChatMessage takes in an arbitrary role parameter. Most of the time, you'll just be dealing with HumanMessage, AIMessage, and SystemMessage.\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import (\nAIMessage,\nHumanMessage,\nSystemMessage\n)\n\nchat = ChatOpenAI(temperature=0)\nchat.predict_messages([HumanMessage(content=\"Translate this sentence from English to French. I love programming.\")])\n# >> AIMessage(content=\"J'aime programmer.\", additional_kwargs={})\nIt is useful to understand how chat models are different from a normal LLM, but it can often be handy to just be able to treat them the same. LangChain makes that easy by also exposing an interface through which you can interact with a chat model as you would a normal LLM. You can access this through the predict interface.\nchat.predict(\"Translate this sentence from English to French. I love programming.\")\n# >> J'aime programmer\nPrompt templatesâ€‹\nMost LLM applications do not pass user input directly into an LLM. Usually they will add the user input to a larger piece of text, called a prompt template, that provides additional context on the specific task at hand.\nIn the previous example, the text we passed to the model contained instructions to generate a company name. For our application, it'd be great if the user only had to provide the description of a company/product, without having to worry about giving the model instructions.\nLLMs\nChat models\nWith PromptTemplates this is easy! In this case our template would be very simple:\nfrom langchain.prompts import PromptTemplate\n\nprompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\nprompt.format(product=\"colorful socks\")\nWhat is a good name for a company that makes colorful socks?\nChainsâ€‹\nNow that we've got a model and a prompt template, we'll want to combine the two. Chains give us a way to link (or chain) together multiple primitives, like models, prompts, and other chains.\nLLMs\nChat models\nThe simplest and most common type of chain is an LLMChain, which passes an input first to a PromptTemplate and then to an LLM. We can construct an LLM chain from our existing model and prompt template.\nUsing this we can replace\nllm.predict(\"What would be a good company name for a company that makes colorful socks?\")\nwith\nfrom langchain.chains import LLMChain\n\nchain = LLMChain(llm=llm, prompt=prompt)\nchain.run(\"colorful socks\")\nThere we go, our first chain! Understanding how this simple chain works will set you up well for working with more complex chains.\nAgentsâ€‹\nOur first chain ran a pre-determined sequence of steps. To handle complex workflows, we need to be able to dynamically choose actions based on inputs.\nAgents do just this: they use a language model to determine which actions to take and in what order. Agents are given access to tools, and they repeatedly choose a tool, run the tool, and observe the output until they come up with a final answer.\nTo load an agent, you need to choose a(n):\nLLM/Chat model: The language model powering the agent.\nTool(s): A function that performs a specific duty. This can be things like: Google Search, Database lookup, Python REPL, other chains. For a list of predefined tools and their specifications, see the Tools documentation.\nAgent name: A string that references a supported agent class. An agent class is largely parameterized by the prompt the language model uses to determine which action to take. Because this notebook focuses on the simplest, highest level API, this only covers using the standard supported agents. If you want to implement a custom agent, see here. For a list of supported agents and their specifications, see here.\nFor this example, we'll be using SerpAPI to query a search engine.\nYou'll need to install the SerpAPI Python package:\npip install google-search-results\nAnd set the SERPAPI_API_KEY environment variable.\nLLMs\nChat models\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.llms import OpenAI\n\n# The language model we're going to use to control the agent.\nllm = OpenAI(temperature=0)\n\n# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\nagent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n\n# Let's test it out!\nagent.run(\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\")\n> Entering new AgentExecutor chain...\n\nThought: I need to find the temperature first, then use the calculator to raise it to the .023 power.\nAction: Search\nAction Input: \"High temperature in SF yesterday\"\nObservation: San Francisco Temperature Yesterday. Maximum temperature yesterday: 57 Â°F (at 1:56 pm) Minimum temperature yesterday: 49 Â°F (at 1:56 am) Average temperature ...\n\nThought: I now have the temperature, so I can use the calculator to raise it to the .023 power.\nAction: Calculator\nAction Input: 57^.023\nObservation: Answer: 1.0974509573251117\n\nThought: I now know the final answer\nFinal Answer: The high temperature in SF yesterday in Fahrenheit raised to the .023 power is 1.0974509573251117.\n\n> Finished chain.\nThe high temperature in SF yesterday in Fahrenheit raised to the .023 power is 1.0974509573251117.\nMemoryâ€‹\nThe chains and agents we've looked at so far have been stateless, but for many applications it's necessary to reference past interactions. This is clearly the case with a chatbot for example, where you want it to understand new messages in the context of past messages.\nThe Memory module gives you a way to maintain application state. The base Memory interface is simple: it lets you update state given the latest run inputs and outputs and it lets you modify (or contextualize) the next input using the stored state.\nThere are a number of built-in memory systems. The simplest of these is a buffer memory which just prepends the last few inputs/outputs to the current input - we will use this in the example below.\nLLMs\nChat models\nfrom langchain import OpenAI, ConversationChain\n\nllm = OpenAI(temperature=0)\nconversation = ConversationChain(llm=llm, verbose=True)\n\nconversation.run(\"Hi there!\")\nhere's what's going on under the hood\n> Entering new chain...\nPrompt after formatting:\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\n\nHuman: Hi there!\nAI:\n\n> Finished chain.\n\n>> 'Hello! How are you today?'\nNow if we run the chain again\nconversation.run(\"I'm doing well! Just having a conversation with an AI.\")\nwe'll see that the full prompt that's passed to the model contains the input and output of our first interaction, along with our latest input\n> Entering new chain...\nPrompt after formatting:\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\n\nHuman: Hi there!\nAI: Hello! How are you today?\nHuman: I'm doing well! Just having a conversation with an AI.\nAI:\n\n> Finished chain.\n\n>> \"That's great! What would you like to talk about?\"",
  "markdown": "## Quickstart\n\n## Installation[â€‹](#installation \"Direct link to Installation\")\n\nTo install LangChain run:\n\n*   Pip\n*   Conda\n\nFor more details, see our [Installation guide](https://python.langchain.com/docs/get_started/installation.html).\n\n## Environment setup[â€‹](#environment-setup \"Direct link to Environment setup\")\n\nUsing LangChain will usually require integrations with one or more model providers, data stores, APIs, etc. For this example, we'll use OpenAI's model APIs.\n\nFirst we'll need to install their Python package:\n\nAccessing the API requires an API key, which you can get by creating an account and heading [here](https://platform.openai.com/account/api-keys). Once we have a key we'll want to set it as an environment variable by running:\n\n```\nexport OPENAI_API_KEY=\"...\"\n```\n\nIf you'd prefer not to set an environment variable you can pass the key in directly via the `openai_api_key` named parameter when initiating the OpenAI LLM class:\n\n```\nfrom langchain.llms import OpenAIllm = OpenAI(openai_api_key=\"...\")\n```\n\n## Building an application[â€‹](#building-an-application \"Direct link to Building an application\")\n\nNow we can start building our language model application. LangChain provides many modules that can be used to build language model applications. Modules can be used as stand-alones in simple applications and they can be combined for more complex use cases.\n\n## LLMs[â€‹](#llms \"Direct link to LLMs\")\n\n#### Get predictions from a language model[â€‹](#get-predictions-from-a-language-model \"Direct link to Get predictions from a language model\")\n\nThe basic building block of LangChain is the LLM, which takes in text and generates more text.\n\nAs an example, suppose we're building an application that generates a company name based on a company description. In order to do this, we need to initialize an OpenAI model wrapper. In this case, since we want the outputs to be MORE random, we'll initialize our model with a HIGH temperature.\n\n```\nfrom langchain.llms import OpenAIllm = OpenAI(temperature=0.9)\n```\n\nAnd now we can pass in text and get predictions!\n\n```\nllm.predict(\"What would be a good company name for a company that makes colorful socks?\")# >> Feetful of Fun\n```\n\n## Chat models[â€‹](#chat-models \"Direct link to Chat models\")\n\nChat models are a variation on language models. While chat models use language models under the hood, the interface they expose is a bit different: rather than expose a \"text in, text out\" API, they expose an interface where \"chat messages\" are the inputs and outputs.\n\nYou can get chat completions by passing one or more messages to the chat model. The response will be a message. The types of messages currently supported in LangChain are `AIMessage`, `HumanMessage`, `SystemMessage`, and `ChatMessage` -- `ChatMessage` takes in an arbitrary role parameter. Most of the time, you'll just be dealing with `HumanMessage`, `AIMessage`, and `SystemMessage`.\n\n```\nfrom langchain.chat_models import ChatOpenAIfrom langchain.schema import (    AIMessage,    HumanMessage,    SystemMessage)chat = ChatOpenAI(temperature=0)chat.predict_messages([HumanMessage(content=\"Translate this sentence from English to French. I love programming.\")])# >> AIMessage(content=\"J'aime programmer.\", additional_kwargs={})\n```\n\nIt is useful to understand how chat models are different from a normal LLM, but it can often be handy to just be able to treat them the same. LangChain makes that easy by also exposing an interface through which you can interact with a chat model as you would a normal LLM. You can access this through the `predict` interface.\n\n```\nchat.predict(\"Translate this sentence from English to French. I love programming.\")# >> J'aime programmer\n```\n\n## Prompt templates[â€‹](#prompt-templates \"Direct link to Prompt templates\")\n\nMost LLM applications do not pass user input directly into an LLM. Usually they will add the user input to a larger piece of text, called a prompt template, that provides additional context on the specific task at hand.\n\nIn the previous example, the text we passed to the model contained instructions to generate a company name. For our application, it'd be great if the user only had to provide the description of a company/product, without having to worry about giving the model instructions.\n\n*   LLMs\n*   Chat models\n\nWith PromptTemplates this is easy! In this case our template would be very simple:\n\n```\nfrom langchain.prompts import PromptTemplateprompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")prompt.format(product=\"colorful socks\")\n```\n\n```\nWhat is a good name for a company that makes colorful socks?\n```\n\n## Chains[â€‹](#chains \"Direct link to Chains\")\n\nNow that we've got a model and a prompt template, we'll want to combine the two. Chains give us a way to link (or chain) together multiple primitives, like models, prompts, and other chains.\n\n*   LLMs\n*   Chat models\n\nThe simplest and most common type of chain is an LLMChain, which passes an input first to a PromptTemplate and then to an LLM. We can construct an LLM chain from our existing model and prompt template.\n\nUsing this we can replace\n\n```\nllm.predict(\"What would be a good company name for a company that makes colorful socks?\")\n```\n\nwith\n\n```\nfrom langchain.chains import LLMChainchain = LLMChain(llm=llm, prompt=prompt)chain.run(\"colorful socks\")\n```\n\nThere we go, our first chain! Understanding how this simple chain works will set you up well for working with more complex chains.\n\n## Agents[â€‹](#agents \"Direct link to Agents\")\n\nOur first chain ran a pre-determined sequence of steps. To handle complex workflows, we need to be able to dynamically choose actions based on inputs.\n\nAgents do just this: they use a language model to determine which actions to take and in what order. Agents are given access to tools, and they repeatedly choose a tool, run the tool, and observe the output until they come up with a final answer.\n\nTo load an agent, you need to choose a(n):\n\n*   LLM/Chat model: The language model powering the agent.\n*   Tool(s): A function that performs a specific duty. This can be things like: Google Search, Database lookup, Python REPL, other chains. For a list of predefined tools and their specifications, see the [Tools documentation](https://python.langchain.com/docs/modules/agents/tools/).\n*   Agent name: A string that references a supported agent class. An agent class is largely parameterized by the prompt the language model uses to determine which action to take. Because this notebook focuses on the simplest, highest level API, this only covers using the standard supported agents. If you want to implement a custom agent, see [here](https://python.langchain.com/docs/modules/agents/how_to/custom_agent.html). For a list of supported agents and their specifications, see [here](https://python.langchain.com/docs/modules/agents/agent_types/).\n\nFor this example, we'll be using SerpAPI to query a search engine.\n\nYou'll need to install the SerpAPI Python package:\n\n```\npip install google-search-results\n```\n\nAnd set the `SERPAPI_API_KEY` environment variable.\n\n*   LLMs\n*   Chat models\n\n```\nfrom langchain.agents import AgentType, initialize_agent, load_toolsfrom langchain.llms import OpenAI# The language model we're going to use to control the agent.llm = OpenAI(temperature=0)# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)# Let's test it out!agent.run(\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\")\n```\n\n```\n> Entering new AgentExecutor chain...Thought: I need to find the temperature first, then use the calculator to raise it to the .023 power.Action: SearchAction Input: \"High temperature in SF yesterday\"Observation: San Francisco Temperature Yesterday. Maximum temperature yesterday: 57 Â°F (at 1:56 pm) Minimum temperature yesterday: 49 Â°F (at 1:56 am) Average temperature ...Thought: I now have the temperature, so I can use the calculator to raise it to the .023 power.Action: CalculatorAction Input: 57^.023Observation: Answer: 1.0974509573251117Thought: I now know the final answerFinal Answer: The high temperature in SF yesterday in Fahrenheit raised to the .023 power is 1.0974509573251117.> Finished chain.\n```\n\n```\nThe high temperature in SF yesterday in Fahrenheit raised to the .023 power is 1.0974509573251117.\n```\n\n## Memory[â€‹](#memory \"Direct link to Memory\")\n\nThe chains and agents we've looked at so far have been stateless, but for many applications it's necessary to reference past interactions. This is clearly the case with a chatbot for example, where you want it to understand new messages in the context of past messages.\n\nThe Memory module gives you a way to maintain application state. The base Memory interface is simple: it lets you update state given the latest run inputs and outputs and it lets you modify (or contextualize) the next input using the stored state.\n\nThere are a number of built-in memory systems. The simplest of these is a buffer memory which just prepends the last few inputs/outputs to the current input - we will use this in the example below.\n\n*   LLMs\n*   Chat models\n\n```\nfrom langchain import OpenAI, ConversationChainllm = OpenAI(temperature=0)conversation = ConversationChain(llm=llm, verbose=True)conversation.run(\"Hi there!\")\n```\n\nhere's what's going on under the hood\n\n```\n> Entering new chain...Prompt after formatting:The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.Current conversation:Human: Hi there!AI:> Finished chain.>> 'Hello! How are you today?'\n```\n\nNow if we run the chain again\n\n```\nconversation.run(\"I'm doing well! Just having a conversation with an AI.\")\n```\n\nwe'll see that the full prompt that's passed to the model contains the input and output of our first interaction, along with our latest input\n\n```\n> Entering new chain...Prompt after formatting:The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.Current conversation:Human: Hi there!AI:  Hello! How are you today?Human: I'm doing well! Just having a conversation with an AI.AI:> Finished chain.>> \"That's great! What would you like to talk about?\"\n```"
},
{
  "url": "https://python.langchain.com/docs/get_started/installation.html",
  "crawl": {
    "loadedUrl": "https://python.langchain.com/docs/get_started/installation.html",
    "loadedTime": "2023-07-21T18:13:54.077Z",
    "referrerUrl": "https://python.langchain.com/en/latest/",
    "depth": 1,
    "httpStatusCode": 404
  },
  "metadata": {
    "canonicalUrl": "https://python.langchain.com/docs/get_started/installation.html",
    "title": "Installation | ðŸ¦œï¸ðŸ”— Langchain",
    "description": null,
    "author": null,
    "keywords": null,
    "languageCode": "en"
  },
  "screenshotUrl": null,
  "text": "Installation\nOfficial releaseâ€‹\nTo install LangChain run:\nPip\nConda\nThat will install the bare minimum requirements of LangChain. A lot of the value of LangChain comes when integrating it with various model providers, datastores, etc. By default, the dependencies needed to do that are NOT installed. However, there are two other ways to install LangChain that do bring in those dependencies.\nTo install modules needed for the common LLM providers, run:\npip install langchain[llms]\nTo install all modules needed for all integrations, run:\npip install langchain[all]\nNote that if you are using zsh, you'll need to quote square brackets when passing them as an argument to a command, for example:\npip install 'langchain[all]'\nFrom sourceâ€‹\nIf you want to install from source, you can do so by cloning the repo and running:",
  "markdown": "## Installation\n\n## Official release[â€‹](#official-release \"Direct link to Official release\")\n\nTo install LangChain run:\n\n*   Pip\n*   Conda\n\nThat will install the bare minimum requirements of LangChain. A lot of the value of LangChain comes when integrating it with various model providers, datastores, etc. By default, the dependencies needed to do that are NOT installed. However, there are two other ways to install LangChain that do bring in those dependencies.\n\nTo install modules needed for the common LLM providers, run:\n\n```\npip install langchain[llms]\n```\n\nTo install all modules needed for all integrations, run:\n\n```\npip install langchain[all]\n```\n\nNote that if you are using `zsh`, you'll need to quote square brackets when passing them as an argument to a command, for example:\n\n```\npip install 'langchain[all]'\n```\n\n## From source[â€‹](#from-source \"Direct link to From source\")\n\nIf you want to install from source, you can do so by cloning the repo and running:"
},
{
  "url": "https://python.langchain.com/docs/get_started/quickstart.html",
  "crawl": {
    "loadedUrl": "https://python.langchain.com/docs/get_started/quickstart.html",
    "loadedTime": "2023-07-21T18:13:55.764Z",
    "referrerUrl": "https://python.langchain.com/en/latest/",
    "depth": 1,
    "httpStatusCode": 404
  },
  "metadata": {
    "canonicalUrl": "https://python.langchain.com/docs/get_started/quickstart.html",
    "title": "Quickstart | ðŸ¦œï¸ðŸ”— Langchain",
    "description": "Installation",
    "author": null,
    "keywords": null,
    "languageCode": "en"
  },
  "screenshotUrl": null,
  "text": "Quickstart\nInstallationâ€‹\nTo install LangChain run:\nPip\nConda\nFor more details, see our Installation guide.\nEnvironment setupâ€‹\nUsing LangChain will usually require integrations with one or more model providers, data stores, APIs, etc. For this example, we'll use OpenAI's model APIs.\nFirst we'll need to install their Python package:\nAccessing the API requires an API key, which you can get by creating an account and heading here. Once we have a key we'll want to set it as an environment variable by running:\nexport OPENAI_API_KEY=\"...\"\nIf you'd prefer not to set an environment variable you can pass the key in directly via the openai_api_key named parameter when initiating the OpenAI LLM class:\nfrom langchain.llms import OpenAI\n\nllm = OpenAI(openai_api_key=\"...\")\nBuilding an applicationâ€‹\nNow we can start building our language model application. LangChain provides many modules that can be used to build language model applications. Modules can be used as stand-alones in simple applications and they can be combined for more complex use cases.\nLLMsâ€‹\nGet predictions from a language modelâ€‹\nThe basic building block of LangChain is the LLM, which takes in text and generates more text.\nAs an example, suppose we're building an application that generates a company name based on a company description. In order to do this, we need to initialize an OpenAI model wrapper. In this case, since we want the outputs to be MORE random, we'll initialize our model with a HIGH temperature.\nfrom langchain.llms import OpenAI\n\nllm = OpenAI(temperature=0.9)\nAnd now we can pass in text and get predictions!\nllm.predict(\"What would be a good company name for a company that makes colorful socks?\")\n# >> Feetful of Fun\nChat modelsâ€‹\nChat models are a variation on language models. While chat models use language models under the hood, the interface they expose is a bit different: rather than expose a \"text in, text out\" API, they expose an interface where \"chat messages\" are the inputs and outputs.\nYou can get chat completions by passing one or more messages to the chat model. The response will be a message. The types of messages currently supported in LangChain are AIMessage, HumanMessage, SystemMessage, and ChatMessage -- ChatMessage takes in an arbitrary role parameter. Most of the time, you'll just be dealing with HumanMessage, AIMessage, and SystemMessage.\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import (\nAIMessage,\nHumanMessage,\nSystemMessage\n)\n\nchat = ChatOpenAI(temperature=0)\nchat.predict_messages([HumanMessage(content=\"Translate this sentence from English to French. I love programming.\")])\n# >> AIMessage(content=\"J'aime programmer.\", additional_kwargs={})\nIt is useful to understand how chat models are different from a normal LLM, but it can often be handy to just be able to treat them the same. LangChain makes that easy by also exposing an interface through which you can interact with a chat model as you would a normal LLM. You can access this through the predict interface.\nchat.predict(\"Translate this sentence from English to French. I love programming.\")\n# >> J'aime programmer\nPrompt templatesâ€‹\nMost LLM applications do not pass user input directly into an LLM. Usually they will add the user input to a larger piece of text, called a prompt template, that provides additional context on the specific task at hand.\nIn the previous example, the text we passed to the model contained instructions to generate a company name. For our application, it'd be great if the user only had to provide the description of a company/product, without having to worry about giving the model instructions.\nLLMs\nChat models\nWith PromptTemplates this is easy! In this case our template would be very simple:\nfrom langchain.prompts import PromptTemplate\n\nprompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\nprompt.format(product=\"colorful socks\")\nWhat is a good name for a company that makes colorful socks?\nChainsâ€‹\nNow that we've got a model and a prompt template, we'll want to combine the two. Chains give us a way to link (or chain) together multiple primitives, like models, prompts, and other chains.\nLLMs\nChat models\nThe simplest and most common type of chain is an LLMChain, which passes an input first to a PromptTemplate and then to an LLM. We can construct an LLM chain from our existing model and prompt template.\nUsing this we can replace\nllm.predict(\"What would be a good company name for a company that makes colorful socks?\")\nwith\nfrom langchain.chains import LLMChain\n\nchain = LLMChain(llm=llm, prompt=prompt)\nchain.run(\"colorful socks\")\nThere we go, our first chain! Understanding how this simple chain works will set you up well for working with more complex chains.\nAgentsâ€‹\nOur first chain ran a pre-determined sequence of steps. To handle complex workflows, we need to be able to dynamically choose actions based on inputs.\nAgents do just this: they use a language model to determine which actions to take and in what order. Agents are given access to tools, and they repeatedly choose a tool, run the tool, and observe the output until they come up with a final answer.\nTo load an agent, you need to choose a(n):\nLLM/Chat model: The language model powering the agent.\nTool(s): A function that performs a specific duty. This can be things like: Google Search, Database lookup, Python REPL, other chains. For a list of predefined tools and their specifications, see the Tools documentation.\nAgent name: A string that references a supported agent class. An agent class is largely parameterized by the prompt the language model uses to determine which action to take. Because this notebook focuses on the simplest, highest level API, this only covers using the standard supported agents. If you want to implement a custom agent, see here. For a list of supported agents and their specifications, see here.\nFor this example, we'll be using SerpAPI to query a search engine.\nYou'll need to install the SerpAPI Python package:\npip install google-search-results\nAnd set the SERPAPI_API_KEY environment variable.\nLLMs\nChat models\nfrom langchain.agents import AgentType, initialize_agent, load_tools\nfrom langchain.llms import OpenAI\n\n# The language model we're going to use to control the agent.\nllm = OpenAI(temperature=0)\n\n# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n\n# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\nagent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n\n# Let's test it out!\nagent.run(\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\")\n> Entering new AgentExecutor chain...\n\nThought: I need to find the temperature first, then use the calculator to raise it to the .023 power.\nAction: Search\nAction Input: \"High temperature in SF yesterday\"\nObservation: San Francisco Temperature Yesterday. Maximum temperature yesterday: 57 Â°F (at 1:56 pm) Minimum temperature yesterday: 49 Â°F (at 1:56 am) Average temperature ...\n\nThought: I now have the temperature, so I can use the calculator to raise it to the .023 power.\nAction: Calculator\nAction Input: 57^.023\nObservation: Answer: 1.0974509573251117\n\nThought: I now know the final answer\nFinal Answer: The high temperature in SF yesterday in Fahrenheit raised to the .023 power is 1.0974509573251117.\n\n> Finished chain.\nThe high temperature in SF yesterday in Fahrenheit raised to the .023 power is 1.0974509573251117.\nMemoryâ€‹\nThe chains and agents we've looked at so far have been stateless, but for many applications it's necessary to reference past interactions. This is clearly the case with a chatbot for example, where you want it to understand new messages in the context of past messages.\nThe Memory module gives you a way to maintain application state. The base Memory interface is simple: it lets you update state given the latest run inputs and outputs and it lets you modify (or contextualize) the next input using the stored state.\nThere are a number of built-in memory systems. The simplest of these is a buffer memory which just prepends the last few inputs/outputs to the current input - we will use this in the example below.\nLLMs\nChat models\nfrom langchain import OpenAI, ConversationChain\n\nllm = OpenAI(temperature=0)\nconversation = ConversationChain(llm=llm, verbose=True)\n\nconversation.run(\"Hi there!\")\nhere's what's going on under the hood\n> Entering new chain...\nPrompt after formatting:\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\n\nHuman: Hi there!\nAI:\n\n> Finished chain.\n\n>> 'Hello! How are you today?'\nNow if we run the chain again\nconversation.run(\"I'm doing well! Just having a conversation with an AI.\")\nwe'll see that the full prompt that's passed to the model contains the input and output of our first interaction, along with our latest input\n> Entering new chain...\nPrompt after formatting:\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\n\nHuman: Hi there!\nAI: Hello! How are you today?\nHuman: I'm doing well! Just having a conversation with an AI.\nAI:\n\n> Finished chain.\n\n>> \"That's great! What would you like to talk about?\"",
  "markdown": "## Quickstart\n\n## Installation[â€‹](#installation \"Direct link to Installation\")\n\nTo install LangChain run:\n\n*   Pip\n*   Conda\n\nFor more details, see our [Installation guide](https://python.langchain.com/docs/get_started/installation.html).\n\n## Environment setup[â€‹](#environment-setup \"Direct link to Environment setup\")\n\nUsing LangChain will usually require integrations with one or more model providers, data stores, APIs, etc. For this example, we'll use OpenAI's model APIs.\n\nFirst we'll need to install their Python package:\n\nAccessing the API requires an API key, which you can get by creating an account and heading [here](https://platform.openai.com/account/api-keys). Once we have a key we'll want to set it as an environment variable by running:\n\n```\nexport OPENAI_API_KEY=\"...\"\n```\n\nIf you'd prefer not to set an environment variable you can pass the key in directly via the `openai_api_key` named parameter when initiating the OpenAI LLM class:\n\n```\nfrom langchain.llms import OpenAIllm = OpenAI(openai_api_key=\"...\")\n```\n\n## Building an application[â€‹](#building-an-application \"Direct link to Building an application\")\n\nNow we can start building our language model application. LangChain provides many modules that can be used to build language model applications. Modules can be used as stand-alones in simple applications and they can be combined for more complex use cases.\n\n## LLMs[â€‹](#llms \"Direct link to LLMs\")\n\n#### Get predictions from a language model[â€‹](#get-predictions-from-a-language-model \"Direct link to Get predictions from a language model\")\n\nThe basic building block of LangChain is the LLM, which takes in text and generates more text.\n\nAs an example, suppose we're building an application that generates a company name based on a company description. In order to do this, we need to initialize an OpenAI model wrapper. In this case, since we want the outputs to be MORE random, we'll initialize our model with a HIGH temperature.\n\n```\nfrom langchain.llms import OpenAIllm = OpenAI(temperature=0.9)\n```\n\nAnd now we can pass in text and get predictions!\n\n```\nllm.predict(\"What would be a good company name for a company that makes colorful socks?\")# >> Feetful of Fun\n```\n\n## Chat models[â€‹](#chat-models \"Direct link to Chat models\")\n\nChat models are a variation on language models. While chat models use language models under the hood, the interface they expose is a bit different: rather than expose a \"text in, text out\" API, they expose an interface where \"chat messages\" are the inputs and outputs.\n\nYou can get chat completions by passing one or more messages to the chat model. The response will be a message. The types of messages currently supported in LangChain are `AIMessage`, `HumanMessage`, `SystemMessage`, and `ChatMessage` -- `ChatMessage` takes in an arbitrary role parameter. Most of the time, you'll just be dealing with `HumanMessage`, `AIMessage`, and `SystemMessage`.\n\n```\nfrom langchain.chat_models import ChatOpenAIfrom langchain.schema import (    AIMessage,    HumanMessage,    SystemMessage)chat = ChatOpenAI(temperature=0)chat.predict_messages([HumanMessage(content=\"Translate this sentence from English to French. I love programming.\")])# >> AIMessage(content=\"J'aime programmer.\", additional_kwargs={})\n```\n\nIt is useful to understand how chat models are different from a normal LLM, but it can often be handy to just be able to treat them the same. LangChain makes that easy by also exposing an interface through which you can interact with a chat model as you would a normal LLM. You can access this through the `predict` interface.\n\n```\nchat.predict(\"Translate this sentence from English to French. I love programming.\")# >> J'aime programmer\n```\n\n## Prompt templates[â€‹](#prompt-templates \"Direct link to Prompt templates\")\n\nMost LLM applications do not pass user input directly into an LLM. Usually they will add the user input to a larger piece of text, called a prompt template, that provides additional context on the specific task at hand.\n\nIn the previous example, the text we passed to the model contained instructions to generate a company name. For our application, it'd be great if the user only had to provide the description of a company/product, without having to worry about giving the model instructions.\n\n*   LLMs\n*   Chat models\n\nWith PromptTemplates this is easy! In this case our template would be very simple:\n\n```\nfrom langchain.prompts import PromptTemplateprompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")prompt.format(product=\"colorful socks\")\n```\n\n```\nWhat is a good name for a company that makes colorful socks?\n```\n\n## Chains[â€‹](#chains \"Direct link to Chains\")\n\nNow that we've got a model and a prompt template, we'll want to combine the two. Chains give us a way to link (or chain) together multiple primitives, like models, prompts, and other chains.\n\n*   LLMs\n*   Chat models\n\nThe simplest and most common type of chain is an LLMChain, which passes an input first to a PromptTemplate and then to an LLM. We can construct an LLM chain from our existing model and prompt template.\n\nUsing this we can replace\n\n```\nllm.predict(\"What would be a good company name for a company that makes colorful socks?\")\n```\n\nwith\n\n```\nfrom langchain.chains import LLMChainchain = LLMChain(llm=llm, prompt=prompt)chain.run(\"colorful socks\")\n```\n\nThere we go, our first chain! Understanding how this simple chain works will set you up well for working with more complex chains.\n\n## Agents[â€‹](#agents \"Direct link to Agents\")\n\nOur first chain ran a pre-determined sequence of steps. To handle complex workflows, we need to be able to dynamically choose actions based on inputs.\n\nAgents do just this: they use a language model to determine which actions to take and in what order. Agents are given access to tools, and they repeatedly choose a tool, run the tool, and observe the output until they come up with a final answer.\n\nTo load an agent, you need to choose a(n):\n\n*   LLM/Chat model: The language model powering the agent.\n*   Tool(s): A function that performs a specific duty. This can be things like: Google Search, Database lookup, Python REPL, other chains. For a list of predefined tools and their specifications, see the [Tools documentation](https://python.langchain.com/docs/modules/agents/tools/).\n*   Agent name: A string that references a supported agent class. An agent class is largely parameterized by the prompt the language model uses to determine which action to take. Because this notebook focuses on the simplest, highest level API, this only covers using the standard supported agents. If you want to implement a custom agent, see [here](https://python.langchain.com/docs/modules/agents/how_to/custom_agent.html). For a list of supported agents and their specifications, see [here](https://python.langchain.com/docs/modules/agents/agent_types/).\n\nFor this example, we'll be using SerpAPI to query a search engine.\n\nYou'll need to install the SerpAPI Python package:\n\n```\npip install google-search-results\n```\n\nAnd set the `SERPAPI_API_KEY` environment variable.\n\n*   LLMs\n*   Chat models\n\n```\nfrom langchain.agents import AgentType, initialize_agent, load_toolsfrom langchain.llms import OpenAI# The language model we're going to use to control the agent.llm = OpenAI(temperature=0)# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)# Let's test it out!agent.run(\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\")\n```\n\n```\n> Entering new AgentExecutor chain...Thought: I need to find the temperature first, then use the calculator to raise it to the .023 power.Action: SearchAction Input: \"High temperature in SF yesterday\"Observation: San Francisco Temperature Yesterday. Maximum temperature yesterday: 57 Â°F (at 1:56 pm) Minimum temperature yesterday: 49 Â°F (at 1:56 am) Average temperature ...Thought: I now have the temperature, so I can use the calculator to raise it to the .023 power.Action: CalculatorAction Input: 57^.023Observation: Answer: 1.0974509573251117Thought: I now know the final answerFinal Answer: The high temperature in SF yesterday in Fahrenheit raised to the .023 power is 1.0974509573251117.> Finished chain.\n```\n\n```\nThe high temperature in SF yesterday in Fahrenheit raised to the .023 power is 1.0974509573251117.\n```\n\n## Memory[â€‹](#memory \"Direct link to Memory\")\n\nThe chains and agents we've looked at so far have been stateless, but for many applications it's necessary to reference past interactions. This is clearly the case with a chatbot for example, where you want it to understand new messages in the context of past messages.\n\nThe Memory module gives you a way to maintain application state. The base Memory interface is simple: it lets you update state given the latest run inputs and outputs and it lets you modify (or contextualize) the next input using the stored state.\n\nThere are a number of built-in memory systems. The simplest of these is a buffer memory which just prepends the last few inputs/outputs to the current input - we will use this in the example below.\n\n*   LLMs\n*   Chat models\n\n```\nfrom langchain import OpenAI, ConversationChainllm = OpenAI(temperature=0)conversation = ConversationChain(llm=llm, verbose=True)conversation.run(\"Hi there!\")\n```\n\nhere's what's going on under the hood\n\n```\n> Entering new chain...Prompt after formatting:The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.Current conversation:Human: Hi there!AI:> Finished chain.>> 'Hello! How are you today?'\n```\n\nNow if we run the chain again\n\n```\nconversation.run(\"I'm doing well! Just having a conversation with an AI.\")\n```\n\nwe'll see that the full prompt that's passed to the model contains the input and output of our first interaction, along with our latest input\n\n```\n> Entering new chain...Prompt after formatting:The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.Current conversation:Human: Hi there!AI:  Hello! How are you today?Human: I'm doing well! Just having a conversation with an AI.AI:> Finished chain.>> \"That's great! What would you like to talk about?\"\n```"
}]